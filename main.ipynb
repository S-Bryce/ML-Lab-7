{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Group\n",
    "Lab Assignment Seven: RNNs\n",
    "Wali Chaudhary, Bryce Shurts, & Alex Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "The dataset is sourced from data.world on Kaggle, and is called \"Emotion Detection from Text\". It was distributed by data.world under a public license according to the source. The dataset is a collection of tweets annotated with emotions attached to each sample.\n",
    "\n",
    "There exist 3 columns, \"tweet_id\", \"sentiment\", and \"content\". Tweet_id represents the identification number of the tweet for querying with the Twitter API, sentiment is the classification of emotion associated with the content, and content is the raw text from the tweet.\n",
    "\n",
    "The dataset contains 13 different emotions, with 40000 records. The dataset is imbalanced, as there are a different number of records per emotion, so this must be addressed in preprocessing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "\n",
    "#### Splitting the data\n",
    "We will be using Stratified K-Fold Cross Validation to split our data into training and testing sets. We chose this method based on the structure of our dataset.\n",
    "\n",
    "The dataset contains imbalanced classes, so we want to ensure that the distribution of the training & testing sets are representative of the overall distribution of classes in the dataset and is beneficial to reducing bias. Features like the sentiment feature are imbalanced with each of the 13 values associated with it having a different distribution across the dataset. Stratified K-Fold Cross Validation ensures this proportionality of distribution, unlike a normal train test split which randomly splits the data into two sets based on a predefined ratio. This can result in a skewed representation of classes in the training & testing datasets not representative of the distribution of the entire dataset.\n",
    "\n",
    "Stratified K-Fold Cross Validation also gives us the advantage of being able to compare different models as each model is trained differently per fold. This will help us in hyper parameter tuning.\n",
    "\n",
    "\n",
    "#### Tokenization methods, Vocabulary, and content length\n",
    "\n",
    "We use the Keras Tokenizer class to tokenize the text data in our dataset. We convert each tweet's text into a sequence of integers, where each integer represents a unique word in the vocabulary. The Tokenizer also takes care of lowercasing, removing punctuation, and handling out-of-vocabulary words.\n",
    "\n",
    "We defined our vocabulary to use all the unique words in \"content\", this was because we didn't want to lose any data as we thought this would also generalize our model better overall. Although this will increase the complexity of our model, and one could also argue that mispellings and grammatical errors can lead to an incorrect classification; we believe that grammatical errors and slang are common ways of emotional expression online. For example, if a user tweets GAAAAAHHHH, that probably means they're very upset versus someone who tweets GAH which may indicate surprise.\n",
    "\n",
    "We decided to keep the length of each sequence to the longest individual sequence by using padding, as this'll help us capture the maximum amount of input for sentiment analysis.\n",
    "\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "For evaluation, we decided to rely on the F1 score, precision, recall, and accuracy. This is because we are performing a classification task on sequences to predict sentiment. Precision is important because it helps us guage the our models True positive predictiveness, and recall helps us identify the number of positive instances correctly identified out of the total amount of positive cases. In our specific case, recall is important because it'll tell us the ratio of how much our classifier was able to identify correct sentiment, like happiness, out the all of the happiness records. Precision will tell us if our evaluation of sentiment was correct.\n",
    "\n",
    "The F1 score balances both of these metrics together, and provides a balanced measure of our models performance. In sentiment analysis, false positives and false negatives are not good because both types of errors can have significant consequences, especially in a system which relies on the classification to provide recommendations or critical information. A high F1-score indicates that our model is making accurate positive predictions while minimizing false positives and false negatives. It is also useful in our case as well since our dataset is imbalanced, because it reduces bias to the majority class by giving a balanced result of the recall and precision to account for both false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T07:18:39.082704Z",
     "end_time": "2023-05-09T07:18:39.095387Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Handle all imports for notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.transform import resize\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T07:19:15.258118Z",
     "end_time": "2023-05-09T07:19:15.346537Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T07:19:16.533095Z",
     "end_time": "2023-05-09T07:19:16.631794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           content                                                            \\\n             count unique                                                top   \nsentiment                                                                      \nanger          110    110                              fuckin'm transtelecom   \nboredom        179    179                                       i'm so tired   \nempty          827    827  @tiffanylue i know  i was listenin to bad habi...   \nenthusiasm     759    759               wants to hang out with friends SOON!   \nfun           1776   1776  Wondering why I'm awake at 7am,writing a new s...   \nhappiness     5209   5194  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...   \nhate          1323   1323  It is so annoying when she starts typing on he...   \nlove          3842   3801  I just received a mothers day card from my lov...   \nneutral       8638   8617  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...   \nrelief        1526   1524  http://snipurl.com/hq0n1 Just printed my mom a...   \nsadness       5165   5160                                       at home sick   \nsurprise      2187   2187                                       Got the news   \nworry         8459   8452                                  i have a headache   \n\n                 \n           freq  \nsentiment        \nanger         1  \nboredom       1  \nempty         1  \nenthusiasm    1  \nfun           1  \nhappiness     4  \nhate          1  \nlove         13  \nneutral       4  \nrelief        2  \nsadness       2  \nsurprise      1  \nworry         3  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"4\" halign=\"left\">content</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>count</th>\n      <th>unique</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n    <tr>\n      <th>sentiment</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>anger</th>\n      <td>110</td>\n      <td>110</td>\n      <td>fuckin'm transtelecom</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>boredom</th>\n      <td>179</td>\n      <td>179</td>\n      <td>i'm so tired</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>empty</th>\n      <td>827</td>\n      <td>827</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>enthusiasm</th>\n      <td>759</td>\n      <td>759</td>\n      <td>wants to hang out with friends SOON!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>fun</th>\n      <td>1776</td>\n      <td>1776</td>\n      <td>Wondering why I'm awake at 7am,writing a new s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>happiness</th>\n      <td>5209</td>\n      <td>5194</td>\n      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>hate</th>\n      <td>1323</td>\n      <td>1323</td>\n      <td>It is so annoying when she starts typing on he...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>love</th>\n      <td>3842</td>\n      <td>3801</td>\n      <td>I just received a mothers day card from my lov...</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>neutral</th>\n      <td>8638</td>\n      <td>8617</td>\n      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>relief</th>\n      <td>1526</td>\n      <td>1524</td>\n      <td>http://snipurl.com/hq0n1 Just printed my mom a...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>sadness</th>\n      <td>5165</td>\n      <td>5160</td>\n      <td>at home sick</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>surprise</th>\n      <td>2187</td>\n      <td>2187</td>\n      <td>Got the news</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>worry</th>\n      <td>8459</td>\n      <td>8452</td>\n      <td>i have a headache</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=0, inplace=True)\n",
    "df.drop(\"tweet_id\", inplace=True, axis=1)\n",
    "\n",
    "df.groupby(df[\"sentiment\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that the sentiment feature is very imbalanced, with each of its 13 values having a different distribution across the dataset. The highest value, happiness with 5209 in count is significantly higher than the lowest count, anger, which has only 110 records.\n",
    "\n",
    "Dropped the \"twitter_id\" column as that was not relevant to the task at hand for classifying sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T07:19:18.421882Z",
     "end_time": "2023-05-09T07:19:18.440063Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the Stratified Shuffle Split object\n",
    "n_splits = 3\n",
    "test_size = 0.2\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "\n",
    "X, y = df.drop(\"sentiment\", inplace=False, axis=1), df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T07:19:19.668299Z",
     "end_time": "2023-05-09T07:19:21.481665Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#tokenize the text, use entire vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "y_np = y.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# save as sequences with integers replacing words\n",
    "tokenizer.fit_on_texts(X[\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(X[\"content\"])\n",
    "final_seqs = pad_sequences(sequences,maxlen=300)\n",
    "num_vocab = len(tokenizer.word_index)+1\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "target = encoder.fit_transform(y_np).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T03:20:22.888748Z",
     "start_time": "2023-05-09T03:20:22.886196Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load in the word embeddings\u001B[39;00m\n\u001B[1;32m      2\u001B[0m word_vectors \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mglove.840B.300d.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m      4\u001B[0m     value \u001B[38;5;241m=\u001B[39m line\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m     word_vectors[value[\u001B[38;5;241m0\u001B[39m]] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(value[\u001B[38;5;241m1\u001B[39m:],dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m     )\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "# Load in the word embeddings\n",
    "word_vectors = {}\n",
    "for line in open(\"glove.840B.300d.txt\"):\n",
    "    value = line.split(' ')\n",
    "    word_vectors[value[0]] = np.array(value[1:],dtype = 'float32')\n",
    "\n",
    "# Apply embeddings to dataset\n",
    "embedding_matrix = np.zeros((num_vocab, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding = word_vectors.get(word)\n",
    "    if embedding is not None:\n",
    "        embedding_matrix[index] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# LSTM RNN\n",
    "model_bi_lstm = Sequential()\n",
    "model_bi_lstm.add(Embedding(num_vocab, 300, weights=[embedding_matrix], input_length=300, trainable=False))\n",
    "model_bi_lstm.add(Bidirectional(CuDNNLSTM(75)))\n",
    "model_bi_lstm.add(Dense(32, activation=\"relu\"))\n",
    "model_bi_lstm.add(Dense(13, activation=\"sigmoid\"))\n",
    "model_bi_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Default padding is to the longest sequence\n",
    "for train_index, test_index in sss.split(sequences, target):\n",
    "    X_train, X_test = final_seqs[train_index], final_seqs[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "    hist = model_bi_lstm.fit(X_train, y_train, epochs=100, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T07:20:00.863068Z",
     "end_time": "2023-05-09T07:20:22.256285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the word embeddings\n",
    "word_vectors = {}\n",
    "for line in open(\"numberbatch-en-17.04b.txt\"):\n",
    "    value = line.split(' ')\n",
    "    word_vectors[value[0]] = np.array(value[1:],dtype = 'float32')\n",
    "\n",
    "# Apply embeddings to dataset\n",
    "embedding_matrix = np.zeros((num_vocab, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding = word_vectors.get(word)\n",
    "    if embedding is not None:\n",
    "        embedding_matrix[index] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential/bidirectional/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed2=0, is_training=true, seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\"]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[sequential/bidirectional/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__inference_train_function_3089]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m X_train, X_test \u001B[38;5;241m=\u001B[39m final_seqs[train_index], final_seqs[test_index]\n\u001B[1;32m     13\u001B[0m y_train, y_test \u001B[38;5;241m=\u001B[39m target[train_index], target[test_index]\n\u001B[0;32m---> 15\u001B[0m hist \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_bi_lstm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     53\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential/bidirectional/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed2=0, is_training=true, seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\"]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[sequential/bidirectional/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__inference_train_function_3089]"
     ]
    }
   ],
   "source": [
    "# LSTM RNN\n",
    "model_bi_lstm = Sequential()\n",
    "model_bi_lstm.add(Embedding(num_vocab, 300, weights=[embedding_matrix], input_length=300, trainable=False))\n",
    "model_bi_lstm.add(Bidirectional(CuDNNLSTM(75)))\n",
    "model_bi_lstm.add(Dense(32, activation=\"relu\"))\n",
    "model_bi_lstm.add(Dense(13, activation=\"sigmoid\"))\n",
    "model_bi_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Default padding is to the longest sequence\n",
    "for train_index, test_index in sss.split(sequences, target):\n",
    "    X_train, X_test = final_seqs[train_index], final_seqs[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "    hist = model_bi_lstm.fit(X_train, y_train, epochs=100, batch_size=256, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "school-ipython-kernel",
   "language": "python",
   "display_name": "school-ipython-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
